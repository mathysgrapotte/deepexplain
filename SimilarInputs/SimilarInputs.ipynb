{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEML Similar Inputs tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ineS3BcpppBz"
      },
      "source": [
        "# Finding Similar Inputs\n",
        "In this section we will take a trained Neural Network and an input, and answer the following questions:\n",
        "* What does the Neural Network output for similar inputs?\n",
        "* Which inputs from the training data have similar activations as this input? In other words, which inputs are considered similar _by the network_?\n",
        "\n",
        "Answering these questions might be useful in domains such as Healthcare and Justice, where similar cases are considered when taking a decision.\n",
        "\n",
        "We will work with the Boston Housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A-4RD-nOKVC",
        "cellView": "form"
      },
      "source": [
        "#@title Train a Neural Network\n",
        "#@markdown (double click here to show the code)\n",
        "#@markdown * Loads the [Boston Housing dataset](https://github.com/scikit-learn/scikit-learn/blob/2beed55847ee70d363bdbfe14ee4401438fba057/sklearn/datasets/data/boston_house_prices.csv) \n",
        "#@markdown * Trains a Neural Network with 1 hidden layer as shown [here](https://colab.research.google.com/github/rpi-techfundamentals/fall2018-materials/blob/master/11-deep-learning2/02-regression-boston-housing-pytorch.ipynb)\n",
        "#@markdown \n",
        "!pip install torch torchvision\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "\n",
        "# Let's change the data to a Panda's Dataframe\n",
        "import pandas as pd\n",
        "boston_df = pd.DataFrame(boston['data'] )\n",
        "#Now add the column names.\n",
        "boston_df.columns = boston['feature_names']\n",
        "#Add the target as PRICE. \n",
        "boston_df['PRICE']= boston['target']\n",
        "\n",
        "#This will throw and error at import if haven't upgraded. \n",
        "# from sklearn.cross_validation  import train_test_split  \n",
        "from sklearn.model_selection  import train_test_split\n",
        "#y is the dependent variable.\n",
        "y = boston_df['PRICE']\n",
        "#As we know, iloc is used to slice the array by index number. Here this is the matrix of \n",
        "#independent variables.\n",
        "X = boston_df.iloc[:,0:13]\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "#Define training hyperprameters.\n",
        "batch_size = 50\n",
        "num_epochs = 400\n",
        "learning_rate = 0.01\n",
        "size_hidden= 100\n",
        "\n",
        "#Calculate some other hyperparameters based on data.  \n",
        "batch_no = len(X_train) // batch_size  #batches\n",
        "cols=X_train.shape[1] #Number of columns in input matrix\n",
        "n_output=1\n",
        "\n",
        "#Create the model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "print(\"Executing the model on :\",device)\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, size_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
        "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
        "        x = self.predict(x)             # linear output\n",
        "        return x\n",
        "net = Net(cols, size_hidden, n_output)\n",
        "\n",
        "#Adam is a specific flavor of gradient decent which is typically better\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')  # this is for regression mean squared loss\n",
        "\n",
        "#Change to numpy arraay. \n",
        "X_train=X_train.values\n",
        "y_train=y_train.values\n",
        "X_test=X_test.values\n",
        "y_test=y_test.values\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from torch.autograd import Variable\n",
        "running_loss = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    #Shuffle just mixes up the dataset between epocs\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    # Mini batch learning\n",
        "    for i in range(batch_no):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        inputs = Variable(torch.FloatTensor(X_train[start:end]))\n",
        "        labels = Variable(torch.FloatTensor(y_train[start:end]))\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        #print(\"outputs\",outputs)\n",
        "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
        "        loss = criterion(outputs, torch.unsqueeze(labels,dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "      \n",
        "    # Only print losses for 5 steps (20%, 40% ... 100%)\n",
        "    if (epoch+1) * 5 % num_epochs == 0:\n",
        "      print('Epoch {}/{}'.format(epoch+1, num_epochs), \"loss: \",running_loss)\n",
        "    running_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tzxLs2bQj_e"
      },
      "source": [
        "def hook(model, input, output):\n",
        "  global activation\n",
        "  activation = output.detach()\n",
        "with torch.no_grad(): # disable gradients to make computations faster\n",
        "  activation = None\n",
        "  net.hidden.register_forward_hook(hook)\n",
        "  net(torch.FloatTensor(X_train))\n",
        "  train_activation = activation\n",
        "  activation = None\n",
        "  net(torch.FloatTensor(X_test))\n",
        "  test_activation = activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkC0uLcDU_se",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f7dacf-133a-492d-e43b-6a40f7fba368"
      },
      "source": [
        "# https://discuss.pytorch.org/t/k-nearest-neighbor-in-pytorch/59695\n",
        "number_of_examples = 5\n",
        "distances = torch.norm(train_activation - test_activation[0:1], dim=1, p=None)\n",
        "knn = distances.topk(number_of_examples, largest=False)\n",
        "print(X_train[knn.indices[0]])\n",
        "print(X_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6.6170e-02 0.0000e+00 3.2400e+00 0.0000e+00 4.6000e-01 5.8680e+00\n",
            " 2.5800e+01 5.2146e+00 4.0000e+00 4.3000e+02 1.6900e+01 3.8244e+02\n",
            " 9.9700e+00]\n",
            "[6.7240e-02 0.0000e+00 3.2400e+00 0.0000e+00 4.6000e-01 6.3330e+00\n",
            " 1.7200e+01 5.2146e+00 4.0000e+00 4.3000e+02 1.6900e+01 3.7521e+02\n",
            " 7.3400e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "duSIdm2vYUvk",
        "outputId": "35f7b1ee-0184-4eec-8e53-20764e0dde7d"
      },
      "source": [
        "result = pd.DataFrame(X_train[knn.indices])\n",
        "result.loc[number_of_examples] = X_test[0]\n",
        "result.columns = boston['feature_names']\n",
        "for i in range(number_of_examples):\n",
        "  result.loc[i, 'PRICE'] = y_train[knn.indices[i]]\n",
        "  result.loc[i, 'NORM'] = torch.norm(train_activation[knn.indices[i]] - test_activation).detach().numpy()\n",
        "result.loc[number_of_examples, 'PRICE'] = y_test[0]\n",
        "print(result.shape)\n",
        "result.head(n=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>NORM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.06617</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.460</td>\n",
              "      <td>5.868</td>\n",
              "      <td>25.8</td>\n",
              "      <td>5.2146</td>\n",
              "      <td>4.0</td>\n",
              "      <td>430.0</td>\n",
              "      <td>16.9</td>\n",
              "      <td>382.44</td>\n",
              "      <td>9.97</td>\n",
              "      <td>19.3</td>\n",
              "      <td>4245.206543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.08387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.437</td>\n",
              "      <td>5.874</td>\n",
              "      <td>36.6</td>\n",
              "      <td>4.5026</td>\n",
              "      <td>5.0</td>\n",
              "      <td>398.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.06</td>\n",
              "      <td>9.10</td>\n",
              "      <td>20.3</td>\n",
              "      <td>4225.157227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.08707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.437</td>\n",
              "      <td>6.140</td>\n",
              "      <td>45.8</td>\n",
              "      <td>4.0905</td>\n",
              "      <td>5.0</td>\n",
              "      <td>398.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>386.96</td>\n",
              "      <td>10.27</td>\n",
              "      <td>20.8</td>\n",
              "      <td>4157.042969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.14476</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.547</td>\n",
              "      <td>5.731</td>\n",
              "      <td>65.2</td>\n",
              "      <td>2.7592</td>\n",
              "      <td>6.0</td>\n",
              "      <td>432.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>391.50</td>\n",
              "      <td>13.61</td>\n",
              "      <td>19.3</td>\n",
              "      <td>4074.811279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.17331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.707</td>\n",
              "      <td>54.0</td>\n",
              "      <td>2.3817</td>\n",
              "      <td>6.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>12.01</td>\n",
              "      <td>21.8</td>\n",
              "      <td>4194.305664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.06724</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.460</td>\n",
              "      <td>6.333</td>\n",
              "      <td>17.2</td>\n",
              "      <td>5.2146</td>\n",
              "      <td>4.0</td>\n",
              "      <td>430.0</td>\n",
              "      <td>16.9</td>\n",
              "      <td>375.21</td>\n",
              "      <td>7.34</td>\n",
              "      <td>22.6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM   ZN  INDUS  CHAS    NOX  ...  PTRATIO       B  LSTAT  PRICE         NORM\n",
              "0  0.06617  0.0   3.24   0.0  0.460  ...     16.9  382.44   9.97   19.3  4245.206543\n",
              "1  0.08387  0.0  12.83   0.0  0.437  ...     18.7  396.06   9.10   20.3  4225.157227\n",
              "2  0.08707  0.0  12.83   0.0  0.437  ...     18.7  386.96  10.27   20.8  4157.042969\n",
              "3  0.14476  0.0  10.01   0.0  0.547  ...     17.8  391.50  13.61   19.3  4074.811279\n",
              "4  0.17331  0.0   9.69   0.0  0.585  ...     19.2  396.90  12.01   21.8  4194.305664\n",
              "5  0.06724  0.0   3.24   0.0  0.460  ...     16.9  375.21   7.34   22.6          NaN\n",
              "\n",
              "[6 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjQFIEcCzLZ"
      },
      "source": [
        "##MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx6TYiAsC1y5",
        "outputId": "56673900-ff2d-4647-cedc-dff5ba050bbf"
      },
      "source": [
        "#@title Training a CNN for MNIST\n",
        "# https://nextjournal.com/gkoehler/pytorch-mnist\n",
        "import torchvision\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 200\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Download the dataset\n",
        "train_dataset = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "network = Net()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      # torch.save(network.state_dict(), '/results/model.pth')\n",
        "      # torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
        "\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  \n",
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3316, Accuracy: 1137/10000 (11%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.360446\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.161339\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.621262\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.704334\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.845895\n",
            "\n",
            "Test set: Avg. loss: 0.1993, Accuracy: 9386/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.460121\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.493157\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.399808\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.358245\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.408614\n",
            "\n",
            "Test set: Avg. loss: 0.1255, Accuracy: 9612/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.204260\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.254541\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.164026\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.153244\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.311787\n",
            "\n",
            "Test set: Avg. loss: 0.1022, Accuracy: 9671/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4csPnKbEyK-",
        "outputId": "aeee763c-0659-463d-f820-3d090c3ee4ac"
      },
      "source": [
        "network.eval() \n",
        "activation = None\n",
        "train_activations = []\n",
        "def hook_fc1(model, input, output):\n",
        "  global activation\n",
        "  activation = torch.cat([input[0].detach(), output.detach()], dim=1)\n",
        "hook_remove_handler = network.fc1.register_forward_hook(hook_fc1)\n",
        "with torch.no_grad(): # disable gradients to make computations faster\n",
        "  train_images = torch.Tensor()\n",
        "  train_activation = torch.Tensor()\n",
        "  train_targets = torch.Tensor()\n",
        "  train_predictions = torch.Tensor()\n",
        "  for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "    activation = None\n",
        "    prediction = network(images).data.max(dim=1, keepdim=True)[1]\n",
        "    train_activation = torch.cat([train_activation, activation])\n",
        "    train_images = torch.cat([train_images, images])\n",
        "    train_targets = torch.cat([train_targets, targets])\n",
        "    train_predictions = torch.cat([train_predictions, prediction])\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Batch: {}/{} ({:.0f}%)'.format(\n",
        "          batch_idx, len(train_loader),\n",
        "          100. * batch_idx / len(train_loader)))\n",
        "      \n",
        "  print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 0/938 (0%)\n",
            "Batch: 200/938 (21%)\n",
            "Batch: 400/938 (43%)\n",
            "Batch: 600/938 (64%)\n",
            "Batch: 800/938 (85%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbqXFOftSdde",
        "outputId": "f5efe735-baab-4a26-a79f-08abd529032a"
      },
      "source": [
        "with torch.no_grad():\n",
        "  activation = None\n",
        "  test_images, test_targets = next(enumerate(test_loader))[1]\n",
        "  test_image = test_images[0:1, :, :, :]\n",
        "  test_target = test_targets[0]\n",
        "  test_prediction = network(test_image).data.max(dim=1, keepdim=True)[1][0].item()\n",
        "  test_activation = activation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "K0m4KJ_9JwH3",
        "outputId": "d5e8c0b7-cba5-44a7-e89e-935a81217afb"
      },
      "source": [
        "number_of_examples = 5\n",
        "distances = torch.norm(train_activation - test_activation, dim=1, p=None)\n",
        "knn = distances.topk(number_of_examples, largest=False)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.tight_layout()\n",
        "plt.subplot(2,5,1)\n",
        "plt.imshow(test_image[0][0], cmap='gray', interpolation='none')\n",
        "plt.title(\"T: {}, P: {}\".format(test_target, test_prediction))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "for i in range(5):\n",
        "  plt.subplot(2,5,i+6)\n",
        "  plt.imshow(train_images[knn.indices[i]][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"T: {}, P: {}\".format(int(train_targets[knn.indices[i]].item()), int(train_predictions[knn.indices[i]].item())))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig.suptitle(\"T: True label, P: Predicted label\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc6hh3LjWLnV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}