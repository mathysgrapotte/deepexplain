{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOB2mmZVLOxpvB66a0PYm/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathysgrapotte/deepexplain/blob/main/intro_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial on explainability for machine learning\n",
        "\n",
        "The past decade has seen exceptional rise in research interest in machine learning paralleled by enormous growth in practical applications. We are in an regime where taming neural networks is quite successful. So we already understand their workings, right?\n",
        "\n",
        "Not at all. \n",
        "\n",
        "This is one of the fundamental problems of the field since the less stakeholders understand a solution the more reluctant they are to use them. To be able to use some technology responsibly, a degree of understanding is a prerequisite. In this context, all members of society should consider themselves affected because in our age, ML solutions are already applied at scale over various digital platforms (substitute your favourite social media company I guess).\n",
        "\n",
        "### What is explainability?\n",
        "\n",
        "To begin with, the term explainability lacks a practical working definition in our context. It clearly means something different for practitioners and the laymen. \n",
        "To make a simple example, imagine a self-driving car crashing into a tree. Of course, the usual question 'why did it happen' has probably no clear answer, since we are not talking about a broken pipe in the brake system but something in the workings of the software. \n",
        "\n",
        "Operative questions could be: will it happen in the future again? What should we do to minimize the risk of another accident? The tests results were quite impressive - is it an exceptional 'outlier' case or we failed to collect sufficient data?\n",
        "\n",
        "We can address explainability on both model level (how does the model work) and prediction level (why did it predict what it did). But the bottom line is that we are usually interested in how the predictions would change if we were to qualitatively modify the input.\n",
        "\n",
        "### What is this tutorial about?\n",
        "\n",
        "In our contribution, we can only scratch the surface of the field. Thus our main goal was to whet the appetite of newcomers for learning more about explainability by introducing some well known (or less known) methods. To be born in mind: explainability itself is in its infancy.\n",
        "\n",
        "The tutorial is rather a textbook with code you can experiment with so the focus is clearly on understanding of the principles of operation rather coding something. Therefore the models we illustrate the methods on are also quite un-sophisticated and clearly nowhere near the state of the art.\n",
        "\n",
        "We imagine our ideal student as a critical, open-minded person who is fasciated about machine learning but also feels that the present state of the field lacks something important.\n",
        "\n"
      ],
      "metadata": {
        "id": "GOkrqQ10XzEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WIrye2t_iXyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}